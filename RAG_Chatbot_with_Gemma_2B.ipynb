{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diyanigam/pdf-upload/blob/treebranch_straight/RAG_Chatbot_with_Gemma_2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install fitz langchain_chroma langchain_huggingface"
      ],
      "metadata": {
        "id": "vJ6L-M-d4w_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import re\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "PERSIST_DIRECTORY = \"./chroma_phi\"\n",
        "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "GEMMA_MODEL_NAME = \"google/gemma-2b-it\"\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)"
      ],
      "metadata": {
        "id": "isQ2y9EbyEny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vectordb = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding_model)\n",
        "# Create a retriever from the vector database.\n",
        "# The 'search_kwargs={\"k\": 7}' means it will retrieve the top 7 most relevant documents.\n",
        "# We're keeping 'k' high to ensure enough relevant sections are retrieved.\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "# --- 3. Load the Gemma 2B LLM ---\n",
        "# Configure BitsAndBytes for 4-bit quantization. This makes the model\n",
        "# significantly smaller in memory and faster, especially on GPUs.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the tokenizer for Gemma 2B. This is crucial for correctly encoding and decoding text.\n",
        "tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_NAME, trust_remote_code=True)\n",
        "# Load the Gemma 2B model with the specified quantization configuration.\n",
        "# 'device_map=\"auto\"' automatically distributes the model across available GPUs.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    GEMMA_MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Gemma's tokenizer might not have a default pad_token. Setting it to eos_token\n",
        "# helps with batching and generation, especially when max_length is involved.\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Create a HuggingFace pipeline for text generation.\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=768, # Increased max_new_tokens for more comprehensive answers\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    temperature=0.7,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# --- Custom LLM Wrapper for LangChain Compatibility ---\n",
        "class CustomHuggingFaceLLM:\n",
        "    def __init__(self, pipeline, tokenizer):\n",
        "        self.pipeline = pipeline\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def invoke(self, prompt_value) -> str:\n",
        "        messages = prompt_value.messages\n",
        "        formatted_prompt = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        result = self.pipeline(formatted_prompt)\n",
        "        generated_text = result[0]['generated_text']\n",
        "\n",
        "        if generated_text.startswith(formatted_prompt):\n",
        "            clean_response = generated_text[len(formatted_prompt):].strip()\n",
        "        else:\n",
        "            clean_response = generated_text.strip()\n",
        "\n",
        "        return clean_response\n",
        "\n",
        "llm = CustomHuggingFaceLLM(llm_pipeline, tokenizer)\n",
        "\n",
        "# --- 4. Define your Prompt Template (HIGHLY IMPROVED FOR STRUCTURED DATA) ---\n",
        "# This template is crucial. It explicitly tells Gemma about the structure of the context.\n",
        "template = \"\"\"\n",
        "You are a helpful and knowledgeable AI assistant. Your purpose is to act as a personal chatbot for a user named [YOUR NAME HERE].\n",
        "You have access to a knowledge base containing specific sections about [YOUR NAME HERE]'s resume, projects, and personal details.\n",
        "\n",
        "Each piece of information in the context is a distinct section, often starting with a clear heading like \"Name of project:\", \"Introduction:\", \"Summary:\", \"Technologies Used:\", \"Objectives:\", \"Problem Statement:\", \"Methodology:\", \"Key Components:\", \"Implementation Details:\", \"Results:\", \"Learnings:\", \"Future Scope:\", \"Linked Resources:\", or similar.\n",
        "\n",
        "Your task is to answer user questions *only* using the provided context.\n",
        "When answering, identify the relevant sections from the context and synthesize the information from those sections to form a comprehensive and accurate answer.\n",
        "If a question asks about a project, look for sections related to that project's name, introduction, summary, technologies, etc.\n",
        "If the provided context does not contain enough information to answer the question, please politely state: \"I apologize, but I don't have enough information in my knowledge base to answer that question.\" Do not try to make up an answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# --- 5. Construct the RAG Chain ---\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | RunnableLambda(llm.invoke)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# --- 6. Chat Function ---\n",
        "def chat_with_gemma(query: str):\n",
        "    response = rag_chain.invoke(query)\n",
        "    return response\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- IMPORTANT: Replace [YOUR NAME HERE] in the template above with your actual name! ---\n",
        "    # Example: template = \"... chatbot for a user named John Doe. ...\"\n",
        "\n",
        "    print(\"\\nWelcome to your personal RAG chatbot with Gemma 2B! Ask me anything about yourself.\")\n",
        "    print(\"Type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nYou: \")\n",
        "        if user_query.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        response = chat_with_gemma(user_query)\n",
        "        print(f\"Bot: {response}\")"
      ],
      "metadata": {
        "id": "OZpKk51zV4Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# This is the directory where your Chroma database is persisted.\n",
        "# Make sure it matches the directory you used when populating the database.\n",
        "PERSIST_DIRECTORY = \"./chroma_phi\"\n",
        "# The embedding model used must be the same as the one used to create the Chroma DB.\n",
        "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "# Using the instruction-tuned version of Gemma 2B for better chat performance.\n",
        "GEMMA_MODEL_NAME = \"google/gemma-2b-it\"\n",
        "\n",
        "# --- 1. Load your Embedding Model ---\n",
        "# This model is used to convert text queries into numerical vectors (embeddings)\n",
        "# to find relevant documents in your Chroma database.\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)"
      ],
      "metadata": {
        "id": "OzFVqsfqyEiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Load your Chroma Vector Database ---\n",
        "# Initialize the Chroma vector database from the persisted directory.\n",
        "# It uses the same embedding function to ensure compatibility.\n",
        "vectordb = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding_model)\n",
        "# Create a retriever from the vector database.\n",
        "# The 'search_kwargs={\"k\": 3}' means it will retrieve the top 3 most relevant documents.\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})"
      ],
      "metadata": {
        "id": "PH2vuiEeyEft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "xPWiiXgFz2g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# --- 3. Load the Gemma 2B LLM ---\n",
        "# Configure BitsAndBytes for 4-bit quantization. This makes the model\n",
        "# significantly smaller in memory and faster, especially on GPUs.\n",
        "# 'load_in_4bit': Loads the model weights in 4-bit precision.\n",
        "# 'bnb_4bit_use_double_quant': Applies a second quantization for even smaller memory footprint.\n",
        "# 'bnb_4bit_quant_type': Specifies the quantization type (NormalFloat 4-bit).\n",
        "# 'bnb_4bit_compute_dtype': Sets the data type for computation (bfloat16 is efficient on modern GPUs).\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the tokenizer for Gemma 2B. This is crucial for correctly encoding and decoding text.\n",
        "tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_NAME, trust_remote_code=True)\n",
        "# Load the Gemma 2B model with the specified quantization configuration.\n",
        "# 'device_map=\"auto\"' automatically distributes the model across available GPUs.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    GEMMA_MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Gemma's tokenizer might not have a default pad_token. Setting it to eos_token\n",
        "# helps with batching and generation, especially when max_length is involved.\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Create a HuggingFace pipeline for text generation.\n",
        "# This simplifies the process of feeding input and getting generated text.\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256, # Limits the maximum length of the generated response.\n",
        "    do_sample=True,     # Enables sampling for more diverse responses.\n",
        "    top_k=50,           # Considers only the top_k most likely tokens.\n",
        "    temperature=0.7,    # Controls the randomness of the generation (lower = more deterministic).\n",
        "    num_return_sequences=1, # Generates only one sequence.\n",
        "    eos_token_id=tokenizer.eos_token_id, # Stops generation when the end-of-sequence token is met.\n",
        "    pad_token_id=tokenizer.pad_token_id  # Uses the pad token for padding inputs.\n",
        ")"
      ],
      "metadata": {
        "id": "U7YxQi6jyEdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Custom LLM Wrapper for LangChain Compatibility ---\n",
        "# LangChain's components expect specific input/output formats.\n",
        "# This wrapper adapts the HuggingFace pipeline to be compatible with LangChain's Runnable interface.\n",
        "class CustomHuggingFaceLLM:\n",
        "    def __init__(self, pipeline, tokenizer):\n",
        "        self.pipeline = pipeline\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def invoke(self, prompt_value) -> str:\n",
        "        # LangChain's ChatPromptTemplate outputs a PromptValue object,\n",
        "        # which contains a list of messages (e.g., SystemMessage, HumanMessage).\n",
        "        # Gemma instruction-tuned models expect a specific chat format,\n",
        "        # which can be generated using the tokenizer's `apply_chat_template`.\n",
        "\n",
        "        # Extract the single message from the PromptValue.\n",
        "        # The ChatPromptTemplate with a simple template creates one HumanMessage.\n",
        "        if not prompt_value.messages or len(prompt_value.messages) > 1:\n",
        "             raise ValueError(\"Expected PromptValue with exactly one message.\")\n",
        "\n",
        "        message = prompt_value.messages[0]\n",
        "\n",
        "        # Create a messages list suitable for the chat template, representing a single user turn.\n",
        "        # We construct a list of dictionaries as expected by apply_chat_template,\n",
        "        # with 'role' and 'content'.\n",
        "        formatted_messages = [\n",
        "            {\"role\": \"user\", \"content\": message.content}\n",
        "        ]\n",
        "\n",
        "        # `add_generation_prompt=True` adds the `<start_of_turn>model\\n` token,\n",
        "        # indicating that the model should start generating its response.\n",
        "        formatted_prompt = self.tokenizer.apply_chat_template(\n",
        "            formatted_messages, # Use the manually created list of messages\n",
        "            tokenize=False, # We want the string, not token IDs yet.\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        # Run the text generation pipeline.\n",
        "        result = self.pipeline(formatted_prompt)\n",
        "        generated_text = result[0]['generated_text']\n",
        "\n",
        "        # The pipeline output will contain the full `formatted_prompt` followed by the generated text.\n",
        "        # We need to remove the input prompt part to get only the model's response.\n",
        "        if generated_text.startswith(formatted_prompt):\n",
        "            clean_response = generated_text[len(formatted_prompt):].strip()\n",
        "        else:\n",
        "            # Fallback in case the generated text doesn't start with the prompt (unlikely but good practice)\n",
        "            clean_response = generated_text.strip()\n",
        "\n",
        "        return clean_response\n",
        "\n",
        "# Instantiate your custom LLM wrapper.\n",
        "llm = CustomHuggingFaceLLM(llm_pipeline, tokenizer)\n",
        "\n",
        "# --- 4. Define your Prompt Template ---\n",
        "# This template guides the LLM on how to use the retrieved context and answer the question.\n",
        "# It will be converted into SystemMessage and HumanMessage objects by ChatPromptTemplate,\n",
        "# which are then formatted by `tokenizer.apply_chat_template` in the CustomHuggingFaceLLM.\n",
        "template = \"\"\"\n",
        "You are a cute girl named Diya. You have all the information about Diya.\n",
        "Use the following context to answer the user's question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# --- 5. Construct the RAG Chain ---\n",
        "# This chain defines the flow of your RAG system:\n",
        "# 1. `{\"context\": retriever, \"question\": RunnablePassthrough()}`:\n",
        "#    - \"context\": The retriever fetches relevant documents based on the user's question.\n",
        "#    - \"question\": The original user's question is passed through.\n",
        "#    This creates a dictionary with 'context' and 'question' keys.\n",
        "# 2. `| prompt`: The dictionary is passed to the ChatPromptTemplate, which formats it\n",
        "#    into a series of messages (PromptValue object) based on the `template`.\n",
        "# 3. `| RunnableLambda(llm.invoke)`: The PromptValue object is then passed to your\n",
        "#    `CustomHuggingFaceLLM`'s `invoke` method, which uses Gemma to generate a response.\n",
        "# 4. `| StrOutputParser()`: The final generated string from Gemma is parsed into a simple string.\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | RunnableLambda(llm.invoke) # Pass the PromptValue object directly to the custom LLM\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# --- 6. Chat Function ---\n",
        "# A simple function to interact with your RAG chatbot.\n",
        "def chat_with_gemma(query: str):\n",
        "    # Invoke the RAG chain with the user's query.\n",
        "    response = rag_chain.invoke(query)\n",
        "    return response\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to your personal RAG chatbot with Gemma 2B! Ask me anything about yourself.\")\n",
        "    print(\"Type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nYou: \")\n",
        "        if user_query.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        # Get response from the chatbot.\n",
        "        response = chat_with_gemma(user_query)\n",
        "\n",
        "        print(f\"Bot: {response}\")"
      ],
      "metadata": {
        "id": "iBj2Wk69yEau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x7ND23WkyEX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AmdRVhiTyEU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jlJn1RJCyESk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}